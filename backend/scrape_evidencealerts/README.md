# EvidenceAlerts AI Summarization Pipeline

This project automates the process of scraping medical article alerts from EvidenceAlerts, generating AI-powered summaries in French using Google Gemini, and exporting the structured data to a Supabase database.

## How it Works

The core logic is orchestrated by the `pipeline.sh` script, which executes three Python scripts in sequence:

1.  **`scrape_evidencealerts.py`**:
    *   Logs into `evidencealerts.com` using credentials defined within the script (**Note: See Configuration section for security warning**).
    *   Navigates to the alerted articles page.
    *   Scrapes links to new articles and extracts metadata (Title, Abstract, Journal, Pub Date, DOI, Categories) by visiting each article page and its corresponding PubMed page.
    *   Translates EvidenceAlerts categories into predefined French equivalents.
    *   Saves the extracted raw data into `data/YYYYMMDD/data.json` (where `YYYYMMDD` is the current date).
    *   Saves the list of scraped EvidenceAlerts article URLs to `links/YYYYMMDD/links.txt`.

2.  **`generate_summaries.py`**:
    *   Reads the `data/YYYYMMDD/data.json` file generated by the scraper.
    *   For each article with an abstract:
        *   Sends the original English title and abstract to the Google Gemini API (`gemini-1.5-flash` by default).
        *   Prompts the AI to translate the title to French and generate a structured French summary (Context, Methodology, Results, Clinical Impact).
        *   Formats the output (including categories, journal, publication date, link) into a structured JSON format.
    *   Saves each processed article summary as an individual JSON file in `summaries/YYYYMMDD/{article_id}.json`. The `article_id` is derived primarily from the EvidenceAlerts URL or fallback identifiers like PMID/DOI hash.

3.  **`export_to_db.py`**:
    *   Connects to the configured Supabase database.
    *   Reads all `.json` files from the `summaries/YYYYMMDD/` directory.
    *   For each summary file:
        *   Checks if an article with the same `link` already exists in the `articles` table.
        *   If the article doesn't exist:
            *   Inserts the article data (title, cleaned content, link, journal, published_at, grade) into the `articles` table.
            *   Retrieves the corresponding French category IDs from the `disciplines` table (using a cache).
            *   Links the newly inserted article to its disciplines in the `article_disciplines` join table.
        *   Skips insertion if the article link already exists.

The `pipeline.sh` script includes checks to ensure that `data.json` exists before running the summary generator, and that the `summaries/YYYYMMDD` directory exists before running the database exporter. It will exit gracefully if no new articles were scraped on a given day.

## Prerequisites

*   Bash environment (Linux, macOS, WSL)
*   Python 3.x (3.7+ recommended for `venv`)
*   Google Chrome browser installed
*   Access credentials for:
    *   EvidenceAlerts
    *   Google AI (Gemini) API Key
    *   Supabase Project (URL and Service Role Key)
*   A Supabase project with the following tables (schema details inferred):
    *   `articles` (columns: `id`, `title`, `content`, `link`, `journal`, `published_at`, `grade`, etc.)
    *   `disciplines` (columns: `id`, `name` - must contain the French category names used in `scrape_evidencealerts.py`)
    *   `article_disciplines` (columns: `article_id`, `discipline_id` - join table)

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repository-url>
    cd <repository-directory>
    ```

2.  **Create and Activate a Virtual Environment:**
    It's highly recommended to use a virtual environment to manage dependencies.
    ```bash
    # Create the virtual environment (e.g., named .venv)
    python3 -m venv .venv

    # Activate the virtual environment
    # On Linux/macOS:
    source .venv/bin/activate
    # On Windows (Git Bash):
    # source .venv/Scripts/activate
    # On Windows (Command Prompt):
    # .venv\Scripts\activate.bat
    # On Windows (PowerShell):
    # .venv\Scripts\Activate.ps1
    ```
    *(You should see `(.venv)` preceding your command prompt after activation.)*

3.  **Install Python dependencies:**
    *(Make sure your virtual environment is activated before running this.)*
    ```bash
    pip install -r requirements.txt
    ```
    *(This installs `selenium`, `google-generativeai`, `python-dotenv`, `python-dateutil`, `supabase`, and `webdriver-manager` within the virtual environment.)*

4.  **Configure Environment Variables:**
    Create a file named `.env` in the project root directory and add your credentials:
    ```dotenv
    # .env file
    GOOGLE_API_KEY=your_google_ai_api_key
    SUPABASE_URL=your_supabase_project_url
    SUPABASE_KEY=your_supabase_service_role_key

    # Optional: Add EvidenceAlerts credentials if modifying the script (Recommended)
    # EVIDENCEALERTS_EMAIL=your_email@example.com
    # EVIDENCEALERTS_PASSWORD=your_password
    ```

5.  **Configure EvidenceAlerts Credentials:**
    *   **⚠️ Security Warning:** The provided `scrape_evidencealerts.py` script has EvidenceAlerts credentials **hardcoded**. This is insecure.
    *   **Recommendation:** Modify `scrape_evidencealerts.py` to read the email and password from environment variables (e.g., `EMAIL = os.getenv("EVIDENCEALERTS_EMAIL")`, `PASSWORD = os.getenv("EVIDENCEALERTS_PASSWORD")`) and add these variables to your `.env` file (as shown in the previous step).

6.  **Database Setup:**
    *   Ensure your Supabase tables (`articles`, `disciplines`, `article_disciplines`) are created.
    *   **Crucially**, populate the `disciplines` table with the French category names exactly as defined in the `CATEGORY_MAP` within `scrape_evidencealerts.py`. The export script uses these names to find the correct `discipline_id`.

7.  **Make pipeline script executable:**
    ```bash
    chmod +x pipeline.sh
    ```

## Usage

1.  **Activate the virtual environment** (if not already active):
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate # Or the appropriate command for your OS
    pip install -r requirements.txt

2. **Set the .env**

3.  **Run the pipeline script** from the project's root directory:
    ```bash
    ./pipeline.sh
    ```

The script will output logs indicating the progress of each step (Scraping, Summarizing, Exporting). Check the console output for success messages or warnings/errors.

## Deactivating the Virtual Environment

When you're finished working on the project, you can deactivate the virtual environment:
```bash
deactivate